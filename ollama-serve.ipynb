{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T15:34:20.802511Z",
     "iopub.status.busy": "2025-01-03T15:34:20.802220Z",
     "iopub.status.idle": "2025-01-03T15:41:22.547761Z",
     "shell.execute_reply": "2025-01-03T15:41:22.546461Z",
     "shell.execute_reply.started": "2025-01-03T15:34:20.802490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraseña cambiada exitosamente.\n",
      "Salida del comando: \n",
      "Instalando OpenSSH server...\n",
      "Creando copia de seguridad de sshd_config...\n",
      "Modificando sshd_config...\n",
      "Reiniciando servicio SSH...\n",
      "Servicio SSH reiniciado exitosamente.\n",
      "Ollama no encontrado. Instalando...\n",
      "Iniciando el servidor Ollama...\n",
      "Ejecutando 'ollama pull deepseek-coder-v2'...\n",
      "Archivo Modelfile creado para el modelo deepseek-coder-v2.\n",
      "Ejecutando: /usr/local/bin/ollama create -f Modelfile deepseek-coder-v2-extra-ctx\n",
      "Archivo Modelfile eliminado para el modelo deepseek-coder-v2.\n",
      "Iniciando túnel SSH con Serveo en el puerto 4924...\n",
      "\u001b[32mForwarding TCP connections from serveo.net:4924\n",
      "Túnel creado exitosamente en serveo.net:4924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5e79020339e0>\u001b[0m in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-5e79020339e0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# Iniciar túnel Serveo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mstart_serveo_tunnel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5e79020339e0>\u001b[0m in \u001b[0;36mstart_serveo_tunnel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Forwarding TCP connections from serveo.net:{puerto_serveo}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Túnel creado exitosamente en serveo.net:{puerto_serveo}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Esperar a que el proceso termine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# USAR OLLAMA API MEDIANTE SERVEO Y REVERSETUNNEL (USAR OLLAMA LOCAL Y USAR MODELOS DEL CUADERNO)\n",
    "#\n",
    "import fileinput\n",
    "import shutil\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import random\n",
    "import re\n",
    "# Configuración inicial\n",
    "OLLAMA_MODELS = \"deepseek-coder-v2\"\n",
    "num_ctx_models = 20000\n",
    "root_password = \"root\"\n",
    "def cambiar_contraseña_root():\n",
    "    # Comando para cambiar la contraseña del usuario root a 'root'\n",
    "    comando = ['chpasswd']\n",
    "\n",
    "    # Entrada para el comando chpasswd en el formato 'usuario:contraseña'\n",
    "    entrada = f'root:{root_password}'\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Ejecutar el comando con la entrada proporcionada\n",
    "        resultado = subprocess.run(comando, input=entrada, text=True, capture_output=True, check=True)\n",
    "        print(\"Contraseña cambiada exitosamente.\")\n",
    "        print(\"Salida del comando:\", resultado.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error al cambiar la contraseña:\", e)\n",
    "        print(\"Salida de error:\", e.stderr)\n",
    "\n",
    "# Llamar a la función para cambiar la contraseña\n",
    "cambiar_contraseña_root()\n",
    "\n",
    "# Ruta del archivo de configuración de OpenSSH\n",
    "config_path = \"/etc/ssh/sshd_config\"\n",
    "backup_path = \"/etc/ssh/sshd_config.bak\"\n",
    "\n",
    "\n",
    "def install_openssh_server():\n",
    "    \"\"\"Instala el servidor OpenSSH si no está instalado.\"\"\"\n",
    "    print(\"Instalando OpenSSH server...\")\n",
    "    subprocess.run([\"apt\", \"install\", \"openssh-server\", \"-y\"], check=True)\n",
    "\n",
    "\n",
    "def edit_sshd_config():\n",
    "    \"\"\"Edita el archivo sshd_config para habilitar acceso root y autenticación por contraseña.\"\"\"\n",
    "    print(\"Creando copia de seguridad de sshd_config...\")\n",
    "    shutil.copy(config_path, backup_path)\n",
    "\n",
    "    print(\"Modificando sshd_config...\")\n",
    "    with fileinput.FileInput(config_path, inplace=True) as file:\n",
    "        for line in file:\n",
    "            if line.strip().startswith(\"#PermitRootLogin\") or line.strip().startswith(\"PermitRootLogin\"):\n",
    "                print(\"PermitRootLogin yes\")  # Permitir acceso como root\n",
    "            elif line.strip().startswith(\"#PasswordAuthentication\") or line.strip().startswith(\"PasswordAuthentication\"):\n",
    "                print(\"PasswordAuthentication yes\")  # Permitir autenticación por contraseña\n",
    "            elif line.strip().startswith(\"#PubkeyAuthentication\") or line.strip().startswith(\"PubkeyAuthentication\"):\n",
    "                print(\"PubkeyAuthentication no\")  # Deshabilitar autenticación con claves públicas\n",
    "            else:\n",
    "                print(line, end=\"\")  # Mantener las líneas no relevantes sin cambios\n",
    "\n",
    "\n",
    "def restart_ssh_service():\n",
    "    \"\"\"Reinicia el servicio SSH para aplicar los cambios.\"\"\"\n",
    "    print(\"Reiniciando servicio SSH...\")\n",
    "    try:\n",
    "        subprocess.run([\"service\", \"ssh\", \"restart\"], check=True)\n",
    "        print(\"Servicio SSH reiniciado exitosamente.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error al reiniciar el servicio SSH: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def run(commands):\n",
    "    for command in commands:\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "\n",
    "def install_ollama():\n",
    "    \"\"\"Instala Ollama si no está instalado.\"\"\"\n",
    "    ollama_installed = subprocess.run([\"which\", \"ollama\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if ollama_installed.returncode != 0:\n",
    "        print(\"Ollama no encontrado. Instalando...\")\n",
    "        commands = [\n",
    "            \"curl -fsSL https://ollama.com/install.sh | sh\",\n",
    "        ]\n",
    "        run(commands)\n",
    "    else:\n",
    "        print(\"Ollama ya está instalado, omitiendo la instalación.\")\n",
    "\n",
    "\n",
    "def start_ollama_server():\n",
    "    \"\"\"Inicia el servidor Ollama en segundo plano.\"\"\"\n",
    "    print(\"Iniciando el servidor Ollama...\")\n",
    "    os.system(\"export OLLAMA_HOST=0.0.0.0\")\n",
    "    subprocess.Popen([\"/usr/local/bin/ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    time.sleep(5)  # Espera para que Ollama esté activo\n",
    "\n",
    "\n",
    "def pull_ollama_models(models):\n",
    "    \"\"\"Ejecuta 'ollama pull' para descargar varios modelos.\"\"\"\n",
    "    model_list = [model.strip() for model in models.split(\",\")]\n",
    "    for model in model_list:\n",
    "        print(f\"Ejecutando 'ollama pull {model}'...\")\n",
    "        subprocess.run([\"/usr/local/bin/ollama\", \"pull\", model], check=True)\n",
    "\n",
    "\n",
    "def start_serveo_tunnel():\n",
    "    \"\"\"Inicia un túnel SSH a través de Serveo y maneja errores de puertos.\"\"\"\n",
    "    while True:\n",
    "        # Generar un puerto aleatorio de 4 dígitos\n",
    "        puerto_serveo = random.randint(1024, 9999)\n",
    "        print(f\"Iniciando túnel SSH con Serveo en el puerto {puerto_serveo}...\")\n",
    "\n",
    "        # Comando para iniciar el túnel\n",
    "        command = [\n",
    "            \"ssh\", \n",
    "            \"-o\", \"StrictHostKeyChecking=no\", \n",
    "            \"-R\", f\"{puerto_serveo}:localhost:22\", \n",
    "            \"serveo.net\"\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Ejecutar el comando y leer la salida en tiempo real\n",
    "            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            for line in iter(process.stdout.readline, \"\"):\n",
    "                print(line.strip())  # Mostrar la salida en tiempo real\n",
    "\n",
    "                # Verificar si el túnel se creó correctamente\n",
    "                if re.search(rf\"Forwarding TCP connections from serveo.net:{puerto_serveo}\", line):\n",
    "                    print(f\"Túnel creado exitosamente en serveo.net:{puerto_serveo}\")\n",
    "                    process.wait()  # Esperar a que el proceso termine\n",
    "                    return\n",
    "\n",
    "                # Si el puerto es menor a 1024\n",
    "                if \"Public forwarding of ports lower than 1024 is disallowed\" in line:\n",
    "                    print(\"Error: El puerto debe ser mayor a 1024. Intentando con otro puerto...\")\n",
    "                    process.terminate()\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurrió un error: {e}. Intentando con otro puerto...\")\n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "\n",
    "\n",
    "def create_and_execute_model_files(models):\n",
    "    \"\"\"Crea y ejecuta archivos Modelfile para cada modelo en la lista.\"\"\"\n",
    "    model_list = [model.strip() for model in models.split(\",\")]\n",
    "    for model in model_list:\n",
    "        modelfile_content = f\"\"\"FROM {model}\\nPARAMETER num_ctx {num_ctx_models}\"\"\"\n",
    "        modelfile_name = \"Modelfile\"\n",
    "\n",
    "        # Crear archivo Modelfile\n",
    "        with open(modelfile_name, \"w\") as modelfile:\n",
    "            modelfile.write(modelfile_content)\n",
    "\n",
    "        print(f\"Archivo {modelfile_name} creado para el modelo {model}.\")\n",
    "\n",
    "        # Ejecutar el comando ollama create\n",
    "        command = f\"/usr/local/bin/ollama create -f {modelfile_name} {model}-extra-ctx\"\n",
    "        print(f\"Ejecutando: {command}\")\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        # Eliminar archivo Modelfile\n",
    "        os.remove(modelfile_name)\n",
    "        print(f\"Archivo {modelfile_name} eliminado para el modelo {model}.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Instalación y configuración inicial\n",
    "    install_openssh_server()\n",
    "    edit_sshd_config()\n",
    "    restart_ssh_service()\n",
    "\n",
    "    install_ollama()\n",
    "\n",
    "    # Iniciar servicios\n",
    "    start_ollama_server()\n",
    "    pull_ollama_models(OLLAMA_MODELS)\n",
    "    os.system(\"clear\")  # Limpia la pantalla después de descargar los modelos\n",
    "\n",
    "    # Crear y ejecutar archivos Modelfile para cada modelo\n",
    "    create_and_execute_model_files(OLLAMA_MODELS)\n",
    "\n",
    "    # Iniciar túnel Serveo\n",
    "    start_serveo_tunnel()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T20:12:28.915512Z",
     "iopub.status.busy": "2024-12-31T20:12:28.915203Z",
     "iopub.status.idle": "2024-12-31T20:14:38.781434Z",
     "shell.execute_reply": "2024-12-31T20:14:38.780624Z",
     "shell.execute_reply.started": "2024-12-31T20:12:28.915487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama no encontrado. Instalando...\n",
      "Descargando e instalando ngrok...\n",
      "Iniciando el servidor Ollama...\n",
      "Ejecutando 'ollama pull llama3.2-vision'...\n",
      "Archivo Modelfile creado para el modelo llama3.2-vision.\n",
      "Ejecutando: /usr/local/bin/ollama create -f Modelfile llama3.2-vision--extra-ctx\n",
      "Archivo Modelfile eliminado para el modelo llama3.2-vision.\n",
      "Configurando ngrok con el token de autenticación...\n",
      "Iniciando ngrok para el puerto 11434...\n",
      "Obteniendo el estado del túnel ngrok...\n",
      "Estado del túnel ngrok:\n",
      "{\"tunnels\":[{\"name\":\"command_line\",\"ID\":\"02d003d63349fef2acfcbfacc503898e\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://ead4-34-29-193-138.ngrok-free.app\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:11434\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# USAR NGROK PARA USAR URL DE OLLAMA SERVE\n",
    "#\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Configuración inicial\n",
    "NGROK_TOKEN = \"NGROK_TOKEN\"\n",
    "OLLAMA_MODELS = \"llama3.2-vision\"\n",
    "NGROK_PORT = 11434\n",
    "num_ctx_models = 20000\n",
    "\n",
    "def run(commands):\n",
    "    for command in commands:\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "\n",
    "def install_ollama():\n",
    "    \"\"\"Instala Ollama si no está instalado.\"\"\"\n",
    "    ollama_installed = subprocess.run([\"which\", \"ollama\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if ollama_installed.returncode != 0:\n",
    "        print(\"Ollama no encontrado. Instalando...\")\n",
    "        commands = [\n",
    "            \"curl -fsSL https://ollama.com/install.sh | sh\",\n",
    "        ]\n",
    "        run(commands)\n",
    "    else:\n",
    "        print(\"Ollama ya está instalado, omitiendo la instalación.\")\n",
    "\n",
    "\n",
    "def install_ngrok():\n",
    "    \"\"\"Descarga e instala ngrok.\"\"\"\n",
    "    print(\"Descargando e instalando ngrok...\")\n",
    "    os.system(\"wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\")\n",
    "    os.system(\"apt install -y tar\")\n",
    "    os.system(\"tar -xvzf ngrok-v3-stable-linux-amd64.tgz -C /usr/local/bin\")\n",
    "\n",
    "\n",
    "def start_ollama_server():\n",
    "    \"\"\"Inicia el servidor Ollama en segundo plano.\"\"\"\n",
    "    print(\"Iniciando el servidor Ollama...\")\n",
    "    os.system(\"export OLLAMA_HOST=0.0.0.0\")\n",
    "    subprocess.Popen([\"/usr/local/bin/ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    time.sleep(5)  # Espera para que Ollama esté activo\n",
    "\n",
    "\n",
    "def pull_ollama_models(models):\n",
    "    \"\"\"Ejecuta 'ollama pull' para descargar varios modelos.\"\"\"\n",
    "    model_list = [model.strip() for model in models.split(\",\")]\n",
    "    for model in model_list:\n",
    "        print(f\"Ejecutando 'ollama pull {model}'...\")\n",
    "        subprocess.run([\"/usr/local/bin/ollama\", \"pull\", model], check=True)\n",
    "\n",
    "\n",
    "def configure_ngrok(token):\n",
    "    \"\"\"Configura ngrok con el token de autenticación.\"\"\"\n",
    "    print(\"Configurando ngrok con el token de autenticación...\")\n",
    "    os.system(f\"ngrok config add-authtoken {token}\")\n",
    "\n",
    "\n",
    "def start_ngrok(port):\n",
    "    \"\"\"Inicia ngrok en el puerto especificado.\"\"\"\n",
    "    print(f\"Iniciando ngrok para el puerto {port}...\")\n",
    "    subprocess.Popen([\"ngrok\", \"http\", str(port), f\"--host-header=localhost:{port}\"],\n",
    "                     stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    time.sleep(5)  # Espera para que ngrok esté activo\n",
    "\n",
    "\n",
    "def create_and_execute_model_files(models):\n",
    "    \"\"\"Crea y ejecuta archivos Modelfile para cada modelo en la lista.\"\"\"\n",
    "    model_list = [model.strip() for model in models.split(\",\")]\n",
    "    for model in model_list:\n",
    "        modelfile_content = f\"\"\"FROM {model}\\nPARAMETER num_ctx {num_ctx_models}\"\"\"\n",
    "        modelfile_name = \"Modelfile\"\n",
    "\n",
    "        # Crear archivo Modelfile\n",
    "        with open(modelfile_name, \"w\") as modelfile:\n",
    "            modelfile.write(modelfile_content)\n",
    "\n",
    "        print(f\"Archivo {modelfile_name} creado para el modelo {model}.\")\n",
    "\n",
    "        # Ejecutar el comando ollama create\n",
    "        command = f\"/usr/local/bin/ollama create -f {modelfile_name} {model}--extra-ctx\"\n",
    "        print(f\"Ejecutando: {command}\")\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        # Eliminar archivo Modelfile\n",
    "        os.remove(modelfile_name)\n",
    "        print(f\"Archivo {modelfile_name} eliminado para el modelo {model}.\")\n",
    "\n",
    "\n",
    "def get_ngrok_status():\n",
    "    \"\"\"Obtiene el estado del túnel ngrok.\"\"\"\n",
    "    print(\"Obteniendo el estado del túnel ngrok...\")\n",
    "    response = os.popen(\"curl -s http://127.0.0.1:4040/api/tunnels\").read()\n",
    "    print(\"Estado del túnel ngrok:\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Instalación y configuración\n",
    "    install_ollama()\n",
    "    install_ngrok()\n",
    "\n",
    "    # Iniciar servicios\n",
    "    start_ollama_server()\n",
    "    pull_ollama_models(OLLAMA_MODELS)\n",
    "    os.system(\"clear\")  # Limpia la pantalla después de descargar los modelos\n",
    "\n",
    "    # Crear y ejecutar archivos Modelfile para cada modelo\n",
    "    create_and_execute_model_files(OLLAMA_MODELS)\n",
    "    configure_ngrok(NGROK_TOKEN)\n",
    "    start_ngrok(NGROK_PORT)\n",
    "    # Obtener estado\n",
    "    get_ngrok_status()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!curl http://127.0.0.1:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Información del Cuaderno\n",
    "\n",
    "Este cuaderno fue adaptado por [HirCoir](https://www.youtube.com/@HirCoir).\n",
    "\n",
    "¡Suscríbete a su canal para más contenido interesante!\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
